{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de80a02e-afe3-4a5d-b1eb-495b4ea83a3a",
   "metadata": {},
   "source": [
    "# Proof-of-Concept for Fine-Tuning Model with PEFT for Daily News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3114a93c-13e1-414c-b612-a6c2924e1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft datasets\n",
    "#!pip install mistral_inference\n",
    "\n",
    "import json\n",
    "import chromadb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, pipeline\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "#from accelerate import dispatch_model\n",
    "\n",
    "# Load spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")  # Adjust if needed\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from newsies.chromadb_client import ChromaDBClient, collections, get_all_headlines, find_ordinal\n",
    "from newsies import targets\n",
    "\n",
    "\n",
    "! mkdir -p ./training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388918ff-80ae-4c82-ac7e-3a0ca523bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection name: ap_news_2025-03-12\n",
      "there are 4142 stories in the collection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NEW YORK (AP) — Harvey Weinstein ‘s #MeToo retrial next month will largely be an abridged version of the original, with one big addition: a charge based on an allegation from a woman who wasn’t a part of the first case.\\nJust how the reprise of the disgraced movie mogul’s prosecution plays out is coming into focus at a hearing Wednesday, where a judge is set to issue rulings on a variety of issues, including the scope of accuser testimony and potential expert witnesses.\\nWeinstein, 72, was in court for the hearing, which started more than a hour late after Judge Curtis Farber met with the prosecution and defense behind closed doors to discuss matters still under seal.\\nThose included a prosecution request that two of the three accusers in the case be allowed to testify about other alleged encounters with Weinstein. They also discussed evidence of the accusers’ sexual history, which prosecutors say should be barred under New York’s Rape Shield Law.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Connect to ChromaDB and Retrieve Data\n",
    "def fetch_news_data():\n",
    "    client = ChromaDBClient()  # Update path\n",
    "    client.collection_name=f\"ap_news_2025-03-12\"\n",
    "    print(f\"collection name: {client.collection.name}\")\n",
    "    collection = client.collection\n",
    "    n  = collection.count()\n",
    "    print(f\"there are {n} stories in the collection\")\n",
    "    results = collection.get(where={\"target\":{\"$eq\":targets.DOCUMENT}}, limit=n)\n",
    "    return results[\"documents\"], results[\"metadatas\"]\n",
    "\n",
    "news_docs, news_metadata = fetch_news_data()\n",
    "\n",
    "news_docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7346dd64-707c-4486-b95e-5027acce73eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_index': 0,\n",
       " 'collection': 'ap_news_2025-03-12',\n",
       " 'date': '2025-03-12',\n",
       " 'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'headline0': 'India’s official Oscar entry, which failed to make the cut, wins big at major Bollywood awards show',\n",
       " 'headline1': 'N/A',\n",
       " 'headline2': 'N/A',\n",
       " 'section0': 'entertainment',\n",
       " 'section1': 'N/A',\n",
       " 'section2': 'N/A',\n",
       " 'target': 'DOCUMENT',\n",
       " 'text': 'JAIPUR, India (AP) — The film that was submitted as India’s official Oscar entry but failed to make the final list of nominees has swept the International Indian Film Academy Awards, which recognize outstanding work in the country’s film industry.\\nDirector Kiran Rao’s critically acclaimed “Laapataa Ladies” — renamed “Lost Ladies” for its Oscar campaign — emerged as the biggest winner at the 2025 IIFA Awards, bagging 10 wins, including best picture and best direction.\\nThe 2023 comedy is about two veiled brides who are accidentally swapped during a train ride, and tackles issues of patriarchy and gender roles, a shift from decades of male-centered mainstream Indian movies.\\n“It’s a rare privilege to win an award for a film like ‘Laapataa Ladies.’ It’s been a wonderful night. It’s a rare privilege to make a film like this,” Rao said in her acceptance speech.',\n",
       " 'uri': './daily_news/20250312/india-iifa-bollywood-film-awards-6f827c8885563b258b4abadf3613baad.txt',\n",
       " 'url': 'https://apnews.com/article/india-iifa-bollywood-film-awards-6f827c8885563b258b4abadf3613baad'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_metadata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb797284-2588-4981-9e35-2203bcd3c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate Question-Answer Pairs using an LLM\n",
    "\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = list(set(ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\"}))\n",
    "    return entities\n",
    "\n",
    "def save_debug_output(prompt, results):\n",
    "    debug_data=[]\n",
    "    debug_data.append({\"qty_prompts\":len(prompt), \"qty_results\":len(results)})\n",
    "    for q,a in zip(prompt, results):\n",
    "        debug_data.append({\"prompt\":q, \"question\":a})\n",
    "    with open(\"debug_missing_questions.jsonl\",\"w\",encoding=\"utf8\") as fh:\n",
    "        fh.write(json.dumps(debug_data, indent=4))\n",
    "\n",
    "\n",
    "def save_qa_to_parquet(qa_data, file_path):\n",
    "    df = pd.DataFrame(qa_data)\n",
    "    df.to_parquet(file_path, index=False)\n",
    "\n",
    "def load_qa_from_parquet(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    return df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4e57f-83be-425b-af92-2b9b86dc8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"doc\":news_docs,\n",
    "    \"uri\": [meta[\"uri\"] for meta in news_metadata],\n",
    "    \"section0\": [meta[\"section0\"] or \"front-page\" for meta in news_metadata],\n",
    "    \"headline0\":[meta[\"headline0\"] for meta in news_metadata],\n",
    "    \"section1\": [meta[\"section1\"] for meta in news_metadata],\n",
    "    \"headline1\":[meta[\"headline1\"] for meta in news_metadata],\n",
    "    \"section2\": [meta[\"section2\"] for meta in news_metadata],\n",
    "    \"headline2\":[meta[\"headline2\"] for meta in news_metadata],\n",
    "    \"ne\": [extract_named_entities(doc) for doc in news_docs]\n",
    "})\n",
    "\n",
    "df1 = df[ (df[\"section1\"]!=\"N/A\") & (df[\"headline1\"] != \"N/A\")]\n",
    "df1 = df1.drop([\"section0\",\"headline0\"], axis=1)\n",
    "df1 = df1.rename(columns={\"section1\":\"section\", \"headline1\":\"headline\"})\n",
    "\n",
    "df2 = df1[ (df1[\"section2\"]!=\"N/A\") & (df1[\"headline2\"] != \"N/A\")]\n",
    "df2 = df2.drop([\"section\",\"headline\"], axis=1)\n",
    "df2 = df2.rename(columns={\"section2\":\"section\", \"headline2\":\"headline\"})\n",
    "\n",
    "df = df.drop([\"section1\",\"headline1\",\"section2\",\"headline2\"],axis=1)\n",
    "df = df.rename(columns={\"section0\":\"section\", \"headline0\":\"headline\"})\n",
    "df1 = df1.drop([\"section2\",\"headline2\"],axis=1)\n",
    "df = pd.concat([df,df1,df2],ignore_index=True, sort=False)\n",
    "\n",
    "dfnoents=df[df[\"ne\"].apply(len)==0].copy()\n",
    "\n",
    "dfne = df[df[\"ne\"].apply(len)>0].copy()\n",
    "dfne = dfne.explode(\"ne\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f2d23e8-87df-483b-b01f-c467e23b102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add prompt\n",
    "dfnoents[\"prompt\"] = dfnoents.apply( lambda row: (\n",
    "                \"Generate three different questions that a reader might ask about the \"\n",
    "                \"following news article. Focus on specific facts, key events, or \"\n",
    "                \"important themes in the article. Each of the three questions should be clear, \"\n",
    "                \"meaningful, and relevant to the article's details. The questions \"\n",
    "                \"should avoid generic inquiries. Ensure the questions cannot be \"\n",
    "                \"answered without reading the article.\\n\"\n",
    "                f\"news article: {row['doc']}\"\n",
    "            ), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3718173-c991-40a7-8117-9a0ed44becb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prompt\n",
    "dfne[\"prompt\"] = dfne.apply(lambda row: (\n",
    "                    f\"Generate a question about '{row['ne']}' that requires knowledge \"\n",
    "                    \"of the following news article. Focus on specific facts, key \"\n",
    "                    \"events, or important themes in the article. The questions should be \"\n",
    "                    \"clear, meaningful, and relevant to the article's details. The questions \"\n",
    "                    \"should avoid generic inquiries. Ensure the question cannot be \"\n",
    "                    \"answered without reading the article.\\n\"\n",
    "                    f\"news article: {row['doc']}\"\n",
    "\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6440db6-d3b2-4c9b-87e1-739d45ee94d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri</th>\n",
       "      <th>section</th>\n",
       "      <th>headline</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./daily_news/20250312/gardening-spring-checkli...</td>\n",
       "      <td>front-page</td>\n",
       "      <td>Spring’s official start is nearly here and the...</td>\n",
       "      <td>Generate three different questions that a read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./daily_news/20250312/movie-review-last-breath...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Movie Review: A gripping deep-sea rescue missi...</td>\n",
       "      <td>Generate three different questions that a read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./daily_news/20250312/health-insurance-deducti...</td>\n",
       "      <td>health</td>\n",
       "      <td>How to deal with fresh health insurance deduct...</td>\n",
       "      <td>Generate three different questions that a read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./daily_news/20250312/masting-trees-acorns-wal...</td>\n",
       "      <td>science</td>\n",
       "      <td>If it seems like there are a lot of acorns thi...</td>\n",
       "      <td>Generate three different questions that a read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./daily_news/20250312/china-economy-sluggish-c...</td>\n",
       "      <td>business</td>\n",
       "      <td>Consumer prices fell in China in February and ...</td>\n",
       "      <td>Generate three different questions that a read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21713</th>\n",
       "      <td>./daily_news/20250312/ultraprocessed-foods-nih...</td>\n",
       "      <td>technology</td>\n",
       "      <td>A National Institutes of Health study aims to ...</td>\n",
       "      <td>Generate a question about 'HHS' that requires ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21714</th>\n",
       "      <td>./daily_news/20250312/ultraprocessed-foods-nih...</td>\n",
       "      <td>technology</td>\n",
       "      <td>A National Institutes of Health study aims to ...</td>\n",
       "      <td>Generate a question about 'the Robert Wood Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21715</th>\n",
       "      <td>./daily_news/20250312/ultraprocessed-foods-nih...</td>\n",
       "      <td>technology</td>\n",
       "      <td>A National Institutes of Health study aims to ...</td>\n",
       "      <td>Generate a question about 'The Associated Pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21716</th>\n",
       "      <td>./daily_news/20250312/ultraprocessed-foods-nih...</td>\n",
       "      <td>technology</td>\n",
       "      <td>A National Institutes of Health study aims to ...</td>\n",
       "      <td>Generate a question about 'AP' that requires k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21717</th>\n",
       "      <td>./daily_news/20250312/ultraprocessed-foods-nih...</td>\n",
       "      <td>technology</td>\n",
       "      <td>A National Institutes of Health study aims to ...</td>\n",
       "      <td>Generate a question about 'the Howard Hughes M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21718 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     uri        section  \\\n",
       "0      ./daily_news/20250312/gardening-spring-checkli...     front-page   \n",
       "1      ./daily_news/20250312/movie-review-last-breath...  entertainment   \n",
       "2      ./daily_news/20250312/health-insurance-deducti...         health   \n",
       "3      ./daily_news/20250312/masting-trees-acorns-wal...        science   \n",
       "4      ./daily_news/20250312/china-economy-sluggish-c...       business   \n",
       "...                                                  ...            ...   \n",
       "21713  ./daily_news/20250312/ultraprocessed-foods-nih...     technology   \n",
       "21714  ./daily_news/20250312/ultraprocessed-foods-nih...     technology   \n",
       "21715  ./daily_news/20250312/ultraprocessed-foods-nih...     technology   \n",
       "21716  ./daily_news/20250312/ultraprocessed-foods-nih...     technology   \n",
       "21717  ./daily_news/20250312/ultraprocessed-foods-nih...     technology   \n",
       "\n",
       "                                                headline  \\\n",
       "0      Spring’s official start is nearly here and the...   \n",
       "1      Movie Review: A gripping deep-sea rescue missi...   \n",
       "2      How to deal with fresh health insurance deduct...   \n",
       "3      If it seems like there are a lot of acorns thi...   \n",
       "4      Consumer prices fell in China in February and ...   \n",
       "...                                                  ...   \n",
       "21713  A National Institutes of Health study aims to ...   \n",
       "21714  A National Institutes of Health study aims to ...   \n",
       "21715  A National Institutes of Health study aims to ...   \n",
       "21716  A National Institutes of Health study aims to ...   \n",
       "21717  A National Institutes of Health study aims to ...   \n",
       "\n",
       "                                                  prompt  \n",
       "0      Generate three different questions that a read...  \n",
       "1      Generate three different questions that a read...  \n",
       "2      Generate three different questions that a read...  \n",
       "3      Generate three different questions that a read...  \n",
       "4      Generate three different questions that a read...  \n",
       "...                                                  ...  \n",
       "21713  Generate a question about 'HHS' that requires ...  \n",
       "21714  Generate a question about 'the Robert Wood Joh...  \n",
       "21715  Generate a question about 'The Associated Pres...  \n",
       "21716  Generate a question about 'AP' that requires k...  \n",
       "21717  Generate a question about 'the Howard Hughes M...  \n",
       "\n",
       "[21718 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([dfnoents, dfne], ignore_index=True, sort=False)\n",
    "df = df.drop([\"ne\",\"doc\"],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692db3e6-5ac8-427d-a5d0-6880163cd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Model & Tokenizer Initialization\n",
    "base_model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Enable torch.compile() for speedup (if available)\n",
    "if torch.__version__ >= \"2.0\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Initialize HF Pipeline\n",
    "qa_generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "# Convert Pandas DataFrame to Hugging Face Dataset\n",
    "def convert_df_to_hf_dataset(df):\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Save to Parquet\n",
    "def save_qa_to_parquet(dataset, batch_id):\n",
    "    batch_file = f\"training_data/qa_dataset_batch_{batch_id}.parquet\"\n",
    "    dataset.to_parquet(batch_file)\n",
    "\n",
    "# Load Pandas DataFrame (Assuming `df` already exists)\n",
    "batch_size = 1000  # Adjust batch size if needed\n",
    "df[\"batch\"] = df.index // batch_size\n",
    "dataset = convert_df_to_hf_dataset(df)\n",
    "\n",
    "# Define processing function for batch generation\n",
    "def generate_questions(batch, batch_id):\n",
    "    batch_prompts = batch[\"prompt\"]\n",
    "\n",
    "    # Generate Questions using Hugging Face Pipeline (Batch Mode)\n",
    "    batch_questions = qa_generator(\n",
    "        batch_prompts,\n",
    "        max_length=100,\n",
    "        truncation=True,\n",
    "        num_return_sequences=3,  # Generate 3 questions per prompt\n",
    "        do_sample=True,  # Introduce randomness for variation\n",
    "        temperature=0.7,  # Adjust temperature for diversity\n",
    "        top_p=0.9,  # Nucleus sampling for more natural responses\n",
    "    )\n",
    "\n",
    "    # Format questions properly\n",
    "    formatted_questions = [\n",
    "        [\n",
    "            (\n",
    "                \"for the next question, return the 'section', \"\n",
    "                \"the 'headline', and the 'URI'\\n\"\n",
    "                f\"question: '{v}'\"\n",
    "            )\n",
    "            for d in qs\n",
    "            for v in d.values()\n",
    "        ]\n",
    "        for qs in batch_questions\n",
    "    ]\n",
    "\n",
    "    batch[\"question\"] = formatted_questions\n",
    "\n",
    "    # Save Each Batch Immediately to Parquet\n",
    "    save_qa_to_parquet(batch, batch_id)\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Apply Efficient Batch Processing\n",
    "dataset = dataset.map(generate_questions, batched=True, batch_size=batch_size, with_indices=True)\n",
    "\n",
    "# Free GPU Memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(datetime.now(), \"All batches processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94da629-81ca-4b77-b3ba-c15b38e6545b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1156ba97-5325-4862-b86a-9c8fe10748b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the QA generation function\n",
    "def generate_qa_pairs(news_docs, news_metadata, batch_size=1000, entity_batch_size=1000):\n",
    "    qa_generator = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=\"google/flan-t5-large\",\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "    total_batches = (len(news_docs) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Initialize the DataFrames for question and entity prompts\n",
    "    question_prompts = pd.DataFrame(\n",
    "        columns=[\"batch\", \"doc\", \"meta\", \"prompt\", \"answer\"]\n",
    "    )\n",
    "    entity_prompts = pd.DataFrame(\n",
    "        columns=[\"batch\", \"doc\", \"meta\", \"entity\", \"prompt\", \"answer\"]\n",
    "    )\n",
    "    print(datetime.now(), \"start processing generated questions for training\")\n",
    "\n",
    "    for batch_start in range(0, len(news_docs), batch_size):\n",
    "        entity_idx = 0\n",
    "        batch_docs = news_docs[batch_start : batch_start + batch_size]\n",
    "        batch_meta = news_metadata[batch_start : batch_start + batch_size]\n",
    "\n",
    "        # Fill in question prompts and entity prompts\n",
    "        for doc, meta in zip(batch_docs, batch_meta):\n",
    "            context = (\n",
    "                f\"'section': {meta['section0'] or 'front-page'}\\t\"\n",
    "                f\"'headline':{meta['headline0']}\\n\"\n",
    "            )\n",
    "            if meta[\"section1\"] != \"N/A\":\n",
    "                context += (\n",
    "                    f\" 'section': {meta['section1']}\\t'headline': {meta['headline1']}\\n\"\n",
    "                )\n",
    "            if meta[\"section2\"] != \"N/A\":\n",
    "                context += (\n",
    "                    f\" 'section': {meta['section2']}\\t'headline': {meta['headline2']}\\n\"\n",
    "                )\n",
    "            context += f\"'URI': {meta['uri']}\\n\"\n",
    "            context += f\"'article': {doc}\"\n",
    "\n",
    "            # Add question prompt for the document\n",
    "            question_prompt = (\n",
    "                \"Generate 3 different questions that a reader might ask about the \"\n",
    "                \"following news article. Focus on specific facts, key events, or \"\n",
    "                \"important themes in the article. The questions should be clear, \"\n",
    "                \"meaningful, and relevant to the article's details. The questions \"\n",
    "                \"should avoid generic inquiries. Ensure the question cannot be \"\n",
    "                \"answered without reading the article.\\n\"\n",
    "                f\"news article: {doc}\"\n",
    "            )\n",
    "\n",
    "            answer = f\"'URI'  {meta['uri']}\\n\"\n",
    "            for i in range(3):\n",
    "                if meta[f\"section{i}\"] != \"N/A\":\n",
    "                    answer += (\n",
    "                        f\"'section' {meta[f\"section{i}\"] or \"front-page\"}\\t\"\n",
    "                        f\"'headline' {meta[f'headline{i}']}\\n\"\n",
    "                    )\n",
    "\n",
    "            question_prompts.loc[len(question_prompts)] = {\n",
    "                \"batch\": f\"{batch_start}\",\n",
    "                \"doc\": doc,\n",
    "                \"meta\": meta,\n",
    "                \"prompt\": question_prompt,\n",
    "                \"answer\": answer,\n",
    "            }\n",
    "\n",
    "            # Entities can number an order of magnitude more than questions -\n",
    "            # so they have to be batched on their own\n",
    "            # Collect entities in the story\n",
    "            entities = [e for e in extract_named_entities(doc) if e != \"AP\"]\n",
    "            for entity in entities:\n",
    "                entity_prompt = (\n",
    "                    f\"Generate a question about '{entity}' that requires knowledge \"\n",
    "                    \"of the following news article. Focus on specific facts, key \"\n",
    "                    \"events, or important themes in the article. The questions should be \"\n",
    "                    \"clear, meaningful, and relevant to the article's details. The questions \"\n",
    "                    \"should avoid generic inquiries. Ensure the question cannot be \"\n",
    "                    \"answered without reading the article.\\n\"\n",
    "                    f\"news article: {doc}\"\n",
    "                )\n",
    "\n",
    "                answer = f\"'URI'  {meta['uri']}\\n\"\n",
    "                for i in range(3):\n",
    "                    if meta[f\"section{i}\"] != \"N/A\":\n",
    "                        answer += (\n",
    "                            f\"'section' {meta[f\"section{i}\"] or \"front-page\"}\\t\"\n",
    "                            f\"'headline' {meta[f'headline{i}']}\\n\"\n",
    "                        )\n",
    "\n",
    "                entity_prompts.loc[len(entity_prompts)] = {\n",
    "                    \"batch\": f\"{batch_start}-{entity_idx // batch_size }\",\n",
    "                    \"doc\": doc,\n",
    "                    \"meta\": meta,\n",
    "                    \"entity\": entity,\n",
    "                    \"prompt\": entity_prompt,\n",
    "                    \"answer\": answer,\n",
    "                }\n",
    "                entity_idx += 1\n",
    "\n",
    "        # At this point, question_prompts and entity_prompts have been built.\n",
    "        # Use the 'batch' field to group the data by batch for efficient processing\n",
    "\n",
    "        # Group question_prompts by the 'batch' field\n",
    "        question_prompts_grouped = question_prompts.groupby(\"batch\")\n",
    "        entity_prompts_grouped = entity_prompts.groupby(\"batch\")\n",
    "\n",
    "        questions = []\n",
    "        # Run qa_generator on question_prompt_ds - this should be batched to batch_size\n",
    "        for batch, batch_data in tqdm(\n",
    "            question_prompts_grouped,\n",
    "            desc=\"Generating questions for question prompts\",\n",
    "            position=2,\n",
    "        ):\n",
    "\n",
    "            # Process the batch separately by extracting 'prompt' field\n",
    "            batch_prompts = batch_data[\"prompt\"].tolist()\n",
    "\n",
    "            # Call qa_generate with the batch of prompts\n",
    "            batch_questions = qa_generator(\n",
    "                batch_prompts, max_length=50, truncation=True\n",
    "            )\n",
    "            batch_questions = [\n",
    "                (\n",
    "                    \"for the next question, return the 'section', \"\n",
    "                    \"the 'headline', and the 'URI'\\n\"\n",
    "                    f\"question: '{q}'\"\n",
    "                )\n",
    "                for q in batch_questions\n",
    "            ]\n",
    "\n",
    "            batch_data[\"question\"] = batch_questions\n",
    "\n",
    "            batch_file = f\"training_data/qa_dataset_batch_{batch}.parquet\"\n",
    "            save_qa_to_parquet(batch_data, batch_file)\n",
    "\n",
    "            questions.extend(batch_questions)\n",
    "\n",
    "        questions = []\n",
    "        for batch, batch_data in tqdm(\n",
    "            entity_prompts_grouped,\n",
    "            desc=\"Generating questions for entity prompts\",\n",
    "            position=2,\n",
    "        ):\n",
    "\n",
    "            # Process the batch separately by extracting 'prompt' field\n",
    "            batch_prompts = batch_data[\"prompt\"].tolist()\n",
    "\n",
    "            # Call qa_generate with the batch of prompts\n",
    "            batch_questions = qa_generator(\n",
    "                batch_prompts, max_length=50, truncation=True\n",
    "            )\n",
    "            batch_questions = [\n",
    "                (\n",
    "                    \"for the next question, return the 'section', \"\n",
    "                    \"the 'headline', and the 'URI'\\n\"\n",
    "                    f\"question: '{q}'\"\n",
    "                )\n",
    "                for q in batch_questions\n",
    "            ]\n",
    "            batch_data[\"question\"] = batch_questions\n",
    "\n",
    "            batch_file = f\"training_data/qa_dataset_batch_{batch}.parquet\"\n",
    "            save_qa_to_parquet(batch_data, batch_file)\n",
    "\n",
    "            questions.extend(batch_questions)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(datetime.now(), \"All batches processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e63502e-6545-43d2-b1c5-f9467fd99de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-15 15:09:47.347008 start processing generated questions for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a158905d2a64bfc867e080182cea7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions for question prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bc864f355648fa8f3b619153a771a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions for entity prompts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_qa_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnews_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mgenerate_qa_pairs\u001b[39m\u001b[34m(news_docs, news_metadata, batch_size, entity_batch_size)\u001b[39m\n\u001b[32m    148\u001b[39m batch_prompts = batch_data[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m].tolist()\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Call qa_generate with the batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m batch_questions = \u001b[43mqa_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m batch_questions = [\n\u001b[32m    155\u001b[39m     (\n\u001b[32m    156\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor the next question, return the \u001b[39m\u001b[33m'\u001b[39m\u001b[33msection\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m batch_questions\n\u001b[32m    161\u001b[39m ]\n\u001b[32m    162\u001b[39m batch_data[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m] = batch_questions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:173\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    145\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    175\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    176\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    177\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    178\u001b[39m     ):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/pipelines/base.py:1349\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1346\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1347\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1348\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m    124\u001b[39m item = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m processed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/pipelines/base.py:1275\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1274\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:202\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    200\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/generation/utils.py:2223\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2215\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2216\u001b[39m         input_ids=input_ids,\n\u001b[32m   2217\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2218\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2219\u001b[39m         **model_kwargs,\n\u001b[32m   2220\u001b[39m     )\n\u001b[32m   2222\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2223\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2224\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2228\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2233\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2234\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2235\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2236\u001b[39m         batch_size=batch_size,\n\u001b[32m   2237\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2242\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/generation/utils.py:3214\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3212\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3214\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3216\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3217\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3218\u001b[39m     outputs,\n\u001b[32m   3219\u001b[39m     model_kwargs,\n\u001b[32m   3220\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3221\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1893\u001b[39m, in \u001b[36mT5ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1890\u001b[39m         decoder_attention_mask = decoder_attention_mask.to(\u001b[38;5;28mself\u001b[39m.decoder.first_device)\n\u001b[32m   1892\u001b[39m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1893\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1894\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1896\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1902\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1903\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1904\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1909\u001b[39m sequence_output = decoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1911\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1124\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1107\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1108\u001b[39m         layer_module.forward,\n\u001b[32m   1109\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1121\u001b[39m         cache_position,\n\u001b[32m   1122\u001b[39m     )\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:725\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[39m\n\u001b[32m    722\u001b[39m     attention_outputs = attention_outputs + cross_attention_outputs[\u001b[32m2\u001b[39m:]\n\u001b[32m    724\u001b[39m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[32m    728\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden_states.dtype == torch.float16:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:338\u001b[39m, in \u001b[36mT5LayerFF.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     forwarded_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     forwarded_states = \u001b[38;5;28mself\u001b[39m.DenseReluDense(forwarded_states)\n\u001b[32m    340\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(forwarded_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:249\u001b[39m, in \u001b[36mT5LayerNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[32m    245\u001b[39m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     variance = \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n\u001b[32m    252\u001b[39m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "generate_qa_pairs(news_docs, news_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd37003-8276-4692-ba84-4a1d689844b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_parquet(project_root+\"/notebooks/training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4dd621b-efbb-4506-b769-0529c97cd4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21253 entries, 0 to 21252\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  21253 non-null  object\n",
      " 1   context   21253 non-null  object\n",
      " 2   answer    21253 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 498.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a6cfbc-18b7-4e6e-a737-7fff252d4fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question    object\n",
      "context     object\n",
      "answer      object\n",
      "dtype: object\n",
      "question    True\n",
      "context     True\n",
      "answer      True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)  # Check column types\n",
    "\n",
    "# Check if all values are strings\n",
    "print(train_df.map(lambda x: isinstance(x, str)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7eafdf4b-fa8e-4613-bd9c-484f22f76f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for the next question, return the 'section', the 'headline', and the 'URI'question: 'What is the name of the movie mogul?'</td>\n",
       "      <td>[section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.</td>\n",
       "      <td>'URI'  ./daily_news/20250312/harvey-weinstein-sexual-misconduct-metoo-retrial-2e8f3c99224cf5ad068e7ef5b5907b8d.txt\\n'section' front-page\\t'headline' Harvey Weinstein appears in court  as judge weighs key rulings for his looming #MeToo retrial\\n'section' politics\\t'headline' Harvey Weinstein appears in court as judge weighs key rulings for his looming #MeToo retrial\\n'section' technology\\t'headline' Harvey Weinstein due in court for key rulings as his #MeToo retrial nears\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for the next question, return the 'section', the 'headline', and the 'URI'question: 'What was the name of the film that was submitted as India's official Oscar entry but failed to make the final list of nominees?'</td>\n",
       "      <td>[section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.</td>\n",
       "      <td>'URI'  ./daily_news/20250312/india-iifa-bollywood-film-awards-6f827c8885563b258b4abadf3613baad.txt\\n'section' entertainment\\t'headline' India’s official Oscar entry, which failed to make the cut, wins big at major Bollywood awards show\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for the next question, return the 'section', the 'headline', and the 'URI'question: 'What is the name of the Iranian journalist?'</td>\n",
       "      <td>[section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.</td>\n",
       "      <td>'URI'  ./daily_news/20250312/iran-murder-plot-trial-masih-alinejad-d17a4b4bad3205c705f4e61f5a288785.txt\\n'section' politics\\t'headline' At trial’s start, prosecutor blames Iran for plot to assassinate outspoken dissident\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for the next question, return the 'section', the 'headline', and the 'URI'question: 'What was the name of the musician who performed at the memorial service?'</td>\n",
       "      <td>[section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.</td>\n",
       "      <td>'URI'  ./daily_news/20250312/roberta-flack-memorial-8b8b7151a5c603db8a87a7b1960c554e.txt\\n'section' front-page\\t'headline' Lauryn Hill and Stevie Wonder delight at Roberta Flack’s ‘Celebration of Life’ memorial\\n'section' entertainment\\t'headline' N/A\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for the next question, return the 'section', the 'headline', and the 'URI'question: 'What was the name of the painting that was attacked?'</td>\n",
       "      <td>[section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.</td>\n",
       "      <td>'URI'  ./daily_news/20250312/greece-art-gallery-vandalism-lawmaker-0ae9ec66bc25c7bf995f65c1d082bfa4.txt\\n'section' entertainment\\t'headline' Greek lawmaker attacks paintings in Athens’ National Gallery, claiming they are offensive\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                question  \\\n",
       "0                                                                                             for the next question, return the 'section', the 'headline', and the 'URI'question: 'What is the name of the movie mogul?'   \n",
       "1  for the next question, return the 'section', the 'headline', and the 'URI'question: 'What was the name of the film that was submitted as India's official Oscar entry but failed to make the final list of nominees?'   \n",
       "2                                                                                      for the next question, return the 'section', the 'headline', and the 'URI'question: 'What is the name of the Iranian journalist?'   \n",
       "3                                                         for the next question, return the 'section', the 'headline', and the 'URI'question: 'What was the name of the musician who performed at the memorial service?'   \n",
       "4                                                                             for the next question, return the 'section', the 'headline', and the 'URI'question: 'What was the name of the painting that was attacked?'   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n",
       "0  [section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.   \n",
       "1  [section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.   \n",
       "2  [section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.   \n",
       "3  [section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.   \n",
       "4  [section]: front-page\\t'headline':Court asked to intervene after email tells USAID workers to destroy classified documents\\n [section]: politics\\t'headline': N/A\\n'URI': ./daily_news/20250312/usaid-trump-burn-order-shred-classified-documents-f042a51c0a9f74c96b0259b51a0d4a83.txt\\n'article': Lawsuits are mounting over the abrupt shutdown of most U.S. foreign assistance and the targeting of the aid agency. In the latest court challenge, Personal Services Contractor Association, representing thousands of contractors now furloughed or fired from USAID, asked the judge to stop any document destruction to preserve evidence.\\nThe email was sent under the name of Erica Carr — the acting executive secretary at USAID — and bears a USAID logo.\\n“Thank you for your assistance in clearing our classified safes and personnel documents” at USAID headquarters in Washington, it begins.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          answer  \n",
       "0  'URI'  ./daily_news/20250312/harvey-weinstein-sexual-misconduct-metoo-retrial-2e8f3c99224cf5ad068e7ef5b5907b8d.txt\\n'section' front-page\\t'headline' Harvey Weinstein appears in court  as judge weighs key rulings for his looming #MeToo retrial\\n'section' politics\\t'headline' Harvey Weinstein appears in court as judge weighs key rulings for his looming #MeToo retrial\\n'section' technology\\t'headline' Harvey Weinstein due in court for key rulings as his #MeToo retrial nears\\n  \n",
       "1                                                                                                                                                                                                                                                  'URI'  ./daily_news/20250312/india-iifa-bollywood-film-awards-6f827c8885563b258b4abadf3613baad.txt\\n'section' entertainment\\t'headline' India’s official Oscar entry, which failed to make the cut, wins big at major Bollywood awards show\\n  \n",
       "2                                                                                                                                                                                                                                                                 'URI'  ./daily_news/20250312/iran-murder-plot-trial-masih-alinejad-d17a4b4bad3205c705f4e61f5a288785.txt\\n'section' politics\\t'headline' At trial’s start, prosecutor blames Iran for plot to assassinate outspoken dissident\\n  \n",
       "3                                                                                                                                                                                                                                  'URI'  ./daily_news/20250312/roberta-flack-memorial-8b8b7151a5c603db8a87a7b1960c554e.txt\\n'section' front-page\\t'headline' Lauryn Hill and Stevie Wonder delight at Roberta Flack’s ‘Celebration of Life’ memorial\\n'section' entertainment\\t'headline' N/A\\n  \n",
       "4                                                                                                                                                                                                                                                       'URI'  ./daily_news/20250312/greece-art-gallery-vandalism-lawmaker-0ae9ec66bc25c7bf995f65c1d082bfa4.txt\\n'section' entertainment\\t'headline' Greek lawmaker attacks paintings in Athens’ National Gallery, claiming they are offensive\\n  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from IPython.display import display\n",
    "train_df.head(5)[[\"question\",\"context\",\"answer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4b670-7327-430a-86e4-069a18276440",
   "metadata": {},
   "source": [
    "### Tokenize and validate data before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0468a71d-dd5f-4397-b532-44fccecd3207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57226a1aa62c45b88a5cfa4fed11ba8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/mpeters/mistral_models/7B-v0.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8b1aad-5793-40cf-ba5a-c341378bfd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e0f14c64044e79a1cb2b5c1320bf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5: Load Model and Apply LoRA Fine-Tuning\n",
    "base_model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312d8f6-8e07-4e68-915b-2c9186f7c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f72e1-0bf2-4b31-a4d1-eb3dedb4bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_and_tokenize_data(df: pd.DataFrame, max_length: int = 512):\n",
    "    \"\"\" Tokenizes the dataset and provides analytics to detect formatting issues. \"\"\"\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    required_columns = [\"question\", \"context\", \"answer\"]\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "    # Convert everything to string\n",
    "    df[\"question\"] = df[\"question\"].fillna(\"\").astype(str)\n",
    "    df[\"context\"] = df[\"context\"].fillna(\"\").astype(str)\n",
    "    df[\"answer\"] = df[\"answer\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Tokenize all fields separately\n",
    "    tokenized_questions = tokenizer(df[\"question\"].tolist(), padding=True, truncation=True, return_tensors=\"np\")\n",
    "    tokenized_contexts = tokenizer(df[\"context\"].tolist(), padding=True, truncation=True, return_tensors=\"np\")\n",
    "    tokenized_answers = tokenizer(df[\"answer\"].tolist(), padding=True, truncation=True, return_tensors=\"np\")\n",
    "\n",
    "    # Extract token lengths\n",
    "    df[\"question_token_len\"] = [len(q) for q in tokenized_questions[\"input_ids\"]]\n",
    "    df[\"context_token_len\"] = [len(c) for c in tokenized_contexts[\"input_ids\"]]\n",
    "    df[\"answer_token_len\"] = [len(a) for a in tokenized_answers[\"input_ids\"]]\n",
    "\n",
    "    # Summary statistics\n",
    "    summary_stats = {\n",
    "        \"Question Length\": df[\"question_token_len\"].describe(),\n",
    "        \"Context Length\": df[\"context_token_len\"].describe(),\n",
    "        \"Answer Length\": df[\"answer_token_len\"].describe(),\n",
    "    }\n",
    "\n",
    "    # Detect potential issues\n",
    "    def detect_anomalies(column_name):\n",
    "        max_allowed = max_length  # Defined by model input size\n",
    "        too_long = df[df[column_name] > max_allowed]\n",
    "        too_short = df[df[column_name] < 5]  # Arbitrary min length threshold\n",
    "        return too_long, too_short\n",
    "\n",
    "    issues = {\n",
    "        \"long_questions\": detect_anomalies(\"question_token_len\"),\n",
    "        \"long_contexts\": detect_anomalies(\"context_token_len\"),\n",
    "        \"long_answers\": detect_anomalies(\"answer_token_len\"),\n",
    "    }\n",
    "\n",
    "    return df, summary_stats, issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a64bf-d9b1-405d-89e7-a108c2e4b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "blessed_df, stats, anomalies = validate_and_tokenize_data(train_df)\n",
    "\n",
    "# Print statistics\n",
    "for key, value in stats.items():\n",
    "    print(f\"\\n🔹 {key}:\\n{value}\\n\")\n",
    "\n",
    "# Print anomalies\n",
    "for issue, (long, short) in anomalies.items():\n",
    "    print(f\"\\n⚠️ {issue.replace('_', ' ').capitalize()}:\")\n",
    "    print(f\" - {len(long)} entries are too long\")\n",
    "    print(f\" - {len(short)} entries are too short\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31d481-dac0-4cd8-a4f0-44ba369b4eb4",
   "metadata": {},
   "source": [
    "### Format and Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c261f83-bf93-4d9d-80e6-2a23518d5225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ee82c2b9774bf2a81de49cb00b268f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21253 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_dataset(qa_dataset, tokenizer):\n",
    "    \"\"\"Ensure tokenizer has a padding token and tokenize dataset.\"\"\"\n",
    "\n",
    "    # Ensure the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use EOS token for padding\n",
    "\n",
    "    def tokenize_sample(sample):\n",
    "        \"\"\"Tokenizes input and output text.\"\"\"\n",
    "        question = str(sample[\"question\"]) if sample[\"question\"] is not None else \"\"\n",
    "        answer = str(sample[\"answer\"]) if sample[\"answer\"] is not None else \"\"\n",
    "\n",
    "        inputs = tokenizer(question, padding=True, truncation=True, max_length=512)\n",
    "        outputs = tokenizer(answer, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        inputs[\"labels\"] = outputs[\"input_ids\"]  # Assign tokenized answers as labels\n",
    "        return inputs\n",
    "\n",
    "    # Drop rows where 'question' is NaN or empty\n",
    "    qa_dataset = qa_dataset.dropna(subset=[\"question\"])\n",
    "    qa_dataset = qa_dataset[qa_dataset[\"question\"].str.strip() != \"\"]  # Remove empty questions\n",
    "\n",
    "    dataset = Dataset.from_pandas(qa_dataset)\n",
    "    tokenized_dataset = dataset.map(tokenize_sample, remove_columns=[\"question\", \"answer\"])\n",
    "\n",
    "    return tokenized_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Apply the function\n",
    "split_dataset = format_dataset(train_df, tokenizer)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f981a-2c7d-4a36-a28f-421e7433e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpeters/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1993531/1538700214.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./news_finetune_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    remove_unused_columns=False,  # Ensure model gets correct inputs\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021522cf-5eba-4c6d-ad2b-35b6b2195b07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`context` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:777\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[32m    780\u001b[39m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[32m    782\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[32m    784\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:739\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[39m\u001b[34m(value, dtype)\u001b[39m\n\u001b[32m    738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.from_numpy(np.array(value))\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/trainer.py:2241\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2239\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/trainer.py:2500\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2498\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2499\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2500\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2501\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2502\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/trainer.py:5180\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches)\u001b[39m\n\u001b[32m   5178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5179\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5180\u001b[39m         batch_samples += [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m   5181\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5182\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/accelerate/data_loader.py:564\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/data/data_collator.py:271\u001b[39m, in \u001b[36mDataCollatorWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m    280\u001b[39m         batch[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m] = batch[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/data/data_collator.py:66\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     69\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3397\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3394\u001b[39m             batch_outputs[key] = []\n\u001b[32m   3395\u001b[39m         batch_outputs[key].append(value)\n\u001b[32m-> \u001b[39m\u001b[32m3397\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:241\u001b[39m, in \u001b[36mBatchEncoding.__init__\u001b[39m\u001b[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[39m\n\u001b[32m    237\u001b[39m     n_sequences = encoding[\u001b[32m0\u001b[39m].n_sequences\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m._n_sequences = n_sequences\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/envs/newsies/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:793\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    788\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_tokens\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    790\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    791\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    792\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    794\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    795\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtruncation=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    797\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m expected).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`context` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292a95c-bb1c-40ea-a8b0-e17a28c18a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 5: Evaluate the Fine-Tuned Model\n",
    "def evaluate_model(sample_question):\n",
    "    inputs = tokenizer(sample_question, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "sample_question = qa_dataset[0][\"question\"]\n",
    "response = evaluate_model(sample_question)\n",
    "print(f\"Q: {sample_question}\\nA: {response}\")\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_debug_output(raw_outputs, file_path=\"debug_missing_questions.jsonl\"):\n",
    "    \"\"\" Save raw LLM responses where questions are missing \"\"\"\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for entry in raw_outputs:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89aa2b-ff21-40be-b49e-75babf2b924e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
